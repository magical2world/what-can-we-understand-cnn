# 一句话描述迁移学习
训练多层架构的一大令人匪夷所思的优点是对于不同的数据集以及不同的任务，模型所学到的特征拥有一定的通用性。例如使用神经网络在ImageNet的识别任务上，其他的物体识别数据集，例如Caltech-101[38]（及[96,154]），其他的识别任务如纹理识别（如[25]），其他的应用如物体检测（如[53]）及视频任务，如对视频中的动作检测（如[41,134,144]）。
多层神经网络的这种对不同数据集及不同任务所提取的特征的通用性，可以归结于它的这种由简单到抽象，由局部到整体的层次性。因此，网络底层趋向于提取在不同任务之间更具有普适性的特征，这使得多层结构有更好的可迁移性。

系统的探索不同网络及不同任务的这种特征的奇妙的可迁移性，可以得到有关迁移学习的几个好的应用实例[150]。首先，仅仅微调网络的高层相对于微调整个网络来说，系统的表现会更好。其次，研究表明任务越多，其迁移的效率就会变得越低。最后，最神奇的是即使是在一个全新的任务下微调的效果也会表现得很好。

近些年，为了使网络的这种迁移性得到增强，一些研究通过将参数的学习问题分为两步，如[3,127]。首先，是快速学习步骤，通常是使用一项特殊的任务来对网络进行优化。然后，网络的参数会经历一个全局的学习，即最小化网络对于不同任务的误差，来对参数进一步进行优化。