#2.2.2 卷积神经网络的不变性
使用卷积神经网络的一大挑战是需要大量的数据来对网络的所有参数进行学习。即使像ImageNet[126]这样拥有超过1百万张图像的大数据集来训练很深的架构也是太小了。为满足神经网络对数据的大量需求，其中一个解决方法是通过人为的对数据进行增强，例如翻转、旋转、抖动等。这些增强方法的优点是使网络变得对于各种变化来说更具有不变性。事实上，这也是AlexNet取得巨大成功的一个主要原因。因此，除了像上一节那样选择容易训练的网络结构外，还要致力于寻找新的模块来使网络训练的更好。特别的，在本节中所介绍的新颖的模块可以直接从原始的数据中学习不变性表示。
在明确解决不变性最大化问题上最著名的当属空间变换网络[76]。尤其是这个网络通过使用新颖的模块来增加了对不重要部分的空间变化不变性，例如在物体识别中由视角变化所导致的不同。这个模型由3个子部分组成：一个localization net，一个grid generator及一个sampler，如图2.12（a）所示。这个操作可以归结为3步。第一，localization net，它通常是一个2层的小的神经网络，以特征图$$U$$为输入来学习变化参数$$\theta$$。例如，这个变换$$T_{\theta}$$可以被定义为一个普通的仿射变换来允许网络去学习转换、定标、旋转、裁剪。其次通过给出来的变换参数及预定大小$$H\times W$$的输出,grid generator会计算对根据公式（2.17）由输入$$U$$所得到的对应坐标$$(x^s_{i},y^s_{i})$$来计算出变换后的坐标$$(x^t_{i},y^t_{i})$$
$$
\binom {x^s_{i}} {x^s_{i}}=\begin{bmatrix}\theta_{11}&\theta_{12}&\theta_{13}\\ \theta_{21}&\theta_{22}&\theta_{23}\end{bmatrix}\begin{pmatrix}x^t_{i} \\ y^t_{i} \\ 1\end{pmatrix}\tag {2.17}
$$
最后sampler通过特征图$$U$$，采样的坐标及像素值$$\left(x^s_{i},y^s_{i}\right)$$来填充得到在$$\left(x^t_{i},y^t_{i}\right)$$输出特征图$$V$$，如图2.12（b）所示。在卷积神经网络的每一层添加一个这样的结构使得模型可以学习到应对输入各种变化的适应性，提高它的不变性从而提高识别准确率。
![](/assets/STN.png)
近期的两种有名的网络模型Deformable卷积神经网络[29]及Active神经网络[78]通过介绍一个灵活的卷积模块来致力于增强卷积神经网络应对几何变换的能力。这些方法的根本思想都是通过避免使用刚性的卷积，这可以使卷积更好的学习到感兴趣的区域（Regions of Inerest,RoI）。这个想法类似于空间变换网络的localization net及grid generator所做的事。为了确定每层的RoIs，卷积块被修改为从一个初始刚性的卷积窗口来学习偏移。具体来说，在给定的刚性窗口上的标准定义开始
$$
y\left(p\right)=\sum_{p_{n}\in R}w(p_{n})x(p-p_{n})\tag {2.18}
$$
这里$$R$$是卷积作用的区域，$$p_{n}$$是$$R$$中的像素，$$w_{p{n}}$$是滤波器的权重。按照下面的公式简添加一个新的项来表示偏移量
$$
y\left(p\right)=\sum_{p_{n}\in R}w(p_{n})x(p-p_{n}-\Delta p_{n})\tag {2.19}
$$
这里$$\Delta p_{n}$$是位置偏移量。现在卷积的最后一步将会是一个可变性的窗口，而不是传统的$$n\times n$$的窗口。为了学习偏移量$$\Delta p_{n}$$，可变形卷积神经网络包含了一个子模块，该模块如图2.13所示，它可以用来学习偏差。不同于空间变换网络需要交替的学习子模块的参数及网络的权重，可变性卷积神经网络能够同时学习权重及偏移量，这方便更快及更容易的应用于其他架构。
![](/assets/deformable.png)